### Introduction
自己回帰フローモデルの変数の順序を因果関係の側面から考え、構造方程式モデルとの類似性を示す。  
自己回帰フローモデルが因果推論タスクに適用できるように、いくつかの制約を課している。  
本論文の新規性は、
- affine normalizing flowに焦点を当て、それが識別可能な因果モデルを定義することを示す。
- よく知られたadditive noise modelを新たに一般化したものであり、その識別可能性の証明が本稿の主要な理論的成果となっています。  
- フローモデルの特性を利用し、以下のように因果探索と因果推論への応用方法を示す。  
    - フローが精確な尤度を効率的に評価できる事実を利用して、尤度比に基づく因果関係の方向性の非線形尺度を提案し、それに伴う最適性
    - フローモデルが精確な因果関係の順序に条件付けされたとき、介入や反事実の問題に精確な答えを得られること  
    - 合成データ、リアルデータでの実験で既存の手法よりも良いパフォーマンスを出すこと

## 前提
### 構造方程式モデル
- $x$: ランダム変数
- $P_x$ : xの分布
- $S_j: x_j = f_j(pa_j, n_j), j=1,...,d$ -(1)  
- $P_n$ : ノイズ変数の分布
- $pa_j$ : $x_j$の親
$P_x$からサンプリングすることは、$P_n$からサンプリングして$S$を伝播させることと同じ  
- $G$ : 因果グラフ。DAG。  
- $\pi$ : 因果グラフ内の順序。$\pi_i < \pi_j$のとき、グラフ内で$x_i$は$x_j$の先祖である(この順序はユニークでなくてもよい)  
因果グラフ$G$が与えられたとき、(1)は以下のように書き換えられる。  
$$x_j = f_j(x_{<\pi(j)}, n_j),j = 1,...,d$$  
ここで、$f_j$はどんな関数でもよいが、(1)の構造方程式モデルは識別可能でなく、識別可能にするためには何らかの制約を課す必要がある。  
additive noise modelはノイズが加法的であると仮定し、その場合の構造方程式モデルは以下
$$x_j = f_j(x_{<\pi(j)})+n_j,j = 1,...,d$$ (3)  

### 自己回帰フローモデル
Normalizing flowモデルは観測変数$x∈R^d$の対数密度を可逆で微分可能な変形Tによって潜在変数$z∈R^d$で表現する。zは単純な分布$p_z(z)$(の積)で表される  
xの密度は以下の式によって得られる。  
$$p_x(x) = p_z(T^{-1}(x))|detJ_{T^{-1}}(x)|$$
普通、$T$, $T^{-1}$はニューラルネットワークである。  
Normalizing flowは異なる変形$T,..., T_k$をつなげて$T = T_1◦...◦T_k$で得られる  
Tのヤコビアンは$T_i$のヤコビアンから求められる。したがって各$T_i$のヤコビアンが計算可能であればよい。  

## 提案モデル
自己回帰フローモデルと構造方程式モデルの類似性を元に、自己回帰フローモデルの因果推論への応用を考える。  
- 3.1: 自己回帰フローモデルと構造方程式モデルを同等に考えるための一般的な条件について示す
- 3.2: 識別可能な因果モデルを提供し、既存のモデル(特に加法性ノイズモデル)を一般化していることを示す。
- 3.3: 因果関係の順序が異なる2つのフローモデルについて、尤度の比に基づいて因果関係の方向性を測定する方法を示す。
- 3.4: 多変量への拡張を示す。  

### 3.1 
自己回帰ノーマライジングフローを因果モデルに適用するための制約  
- (1) Fixed ordering
    - 自己回帰変換を複数の変換の積であらわすとき、入力変数の順序はどの変換でも同じとする
- Affine/additive transformations
    - 変換器$\tau_j$はAffine型である  
    modelがuniversal density estimator(任意の分布を推定できる)であってはならない
    $$\tau_j(u,v)=e^{s_j(v)}u + t_j(v)$$ (5)  
    加法変換は、$s_j = 0$の特殊なケースである。

### 3.2
- Theorem 1
    - $x = (x_1, x_2)$が式(6)で表されるモデルに従っており、$z_1$, $z_2$が統計的に独立しており、原因と結果を結びつける関数$t_j$が非線形で可逆であると仮定する。  
    $z_1$, $z_2$がガウス分布のとき、モデルは識別可能($\pi$が一意に決定できる)。  
    また、$s_1 = s_2 = 0$の場合、$z_1$, $z_2$の分布によらずモデルは識別可能である。(Hoyer et al, 2009)

### 3.3 二変量でのモデル
因果探索を尤度比検定($x_1 → x_2$と$x_2 → x_1$どちらの尤度が大きいか)と見る。

### 3.4 多変量への拡張
- 3.3の手法を多変量で考えうる順序に試して最も尤度が高いものを採用する→変数の数に対して指数関数的に順序の組み合わせが増える
- 伝統的な手法(PCアルゴリズム)と二変量での尤度比検定を組み合わせる
- 多変量の組に因果関係を発見できる初めてのモデル

## 5 EXPERIMENTS
### 5.1
提案手法；CAREFL
    - CAREFL
    - CAREFL-NS((6)式の$s_j=0$)
比較手法;
    - 線形尤度比法(Hyvarinen, Smith, 2013)
    - 加法的ノイズモデル(Hoyer et al, 2009)
    - 回帰誤差因果推論(Bloebaum et al, 2018)


## どんなもの？
自己回帰フローと構造方程式モデルの類似性に着目し、自己回帰フローを因果推論タスクに応用
## 先行研究と比べてどこがすごい？

## 技術や手法のキモはどこ？
自己回帰フローを構造方程式モデルと対応させるために2つの制約を課す。  
1.Fixed ordering : 自己回帰フローの変換器$T(=T_1◦T_2◦...◦T_k)$のすべてのサブ変換器$T_i$で、入力変数の順序($\pi$)は同じ。
2.Afiine/additive transformations : 
## どうやって有効だと検証した？
提案手法：CAREFL
- CAREFL
- CAREFL-NS : $s_j=0$
合成データと現実のデータに対して、3つの既存手法との比較
    - linear likelihood ratio method 線形尤度比法(Hyvarinen, Smith, 2013)
    - additive noise model(ANM) 加法的ノイズモデル(Hoyer et al, 2009)
        - ガウス過程回帰とニューラルネットワーク
    - Regression Error Causal Inference(RECI) 回帰誤差因果推論(Bloebaum et al, 2018)
### 二変量合成データ
- $x_1$, $x_2$に線形、非線形の因果関係を持たせて、サンプルサイズを25~500までの場合で実験。すべての実験においてCAREFLの精度がよかった。実験を通してネットワーク構造やパラメータは同じであり、提案手法はデータの真の因果関係に依存しないことを強調
### 二値変量リアルデータcause-effectt pairs
### 脳波の時系列データ(EEG)  
最初の150~500時点を取り出し、118個のチャンネル(特徴量)の時系列をランダムに逆転させる。$x_t→x_{t+1}$か$x_{t+1}→x_t$かを推論するタスク
### 介入(合成データ)
### 介入(fMRIデータ)
26人の難治性てんかん患者の脳に電極を埋め込み、安静時と電極の刺激時のデータ  
そのうち帯状回付近に電極がある16人のデータを使用  
患者ごとに脳の2つの部位(CG, HG)のデータを二変量時系列データとする。二つの部位にはCG→HGという因果関係が知られている  
安静時のデータを学習データとしてモデルを学習する。得られたもでるについて、CGへの電気刺激を介入としてCGのデータからHGの活動を予測する。
### 反事実

## 議論はある？
## 次に読むべき論文は？
