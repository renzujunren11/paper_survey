### Graph Convolutional Network
- スパースされたグラフにグラフCNを実行し、シーンの特徴量を得る
- シーンの直近の隣接ノードの情報のみをエンコードするので，畳み込みは1層になる
- すべてのシーンには自己ループが追加される
    - これは、シーンsiの表現自体が近傍表現tiに影響を与えることを意味します。  
$$
t_i=f(\frac{1}{P_i∪\{s_i\}}\sum_{j∈P_i∪\{s_i\}}(W_gc_j+b))
$$  
f(.):非線形の活性化関数、RELUなど  
ベクトルc:シーンの内容を（脚本全体とその相対位置との関係で）表す。  
- 脚本をBiLSTMによってシーケンス(v1, v2, ..., vN)にエンコードし、LSTMのforwardとbackward隠れ層を結合することで、文脈表現cを得る。  
- グラフ畳み込みはLSTMの先頭で行われる。  
- 文脈付けされたシーン表現cが長い範囲のシーン間の関係を捉る一方で、1層GCNは直近の隣接ノードのみ考慮する
- Fig2(e)は、我々のGCNを示し、隣接表現tの計算を示している。
- 最後に、近傍表現 ti とコンテンツ表現 ci を連結して、各シーン si のエンコーディングを得る。このベクトルは、確率 $p(y_{it}|s_i , D)$ を出力する単一のニューロンに与えられます。

### Model Training
- ここまではTPラベルのある脚本があることを前提としている
- 映画を観て、脚本を読み、TPラベルを付けるのは大変
- そういったデータセットは知る限り以下の一つ
- TRIPODデータセット
    - 物語構造を分析するためのデータセット
    - 122の映画と17000のシーンを含み、38には広く知られた文章のアノテーションがあり評価に使用した
    - 映画の説明文(叙述?)の文章にTPラベルがついたデータセット
- そのため、広く認められた(gold-standard)TP文がついた説明文と対応するシーンを入力に、$p(y_{it}|s_i , D)$を出力として、教師モデルを学習させ、t番目のTP文の意味を調査した。
- 教師モデルから生成されたTP固有の事後分布は、シーンのみを入力とするモデルの学習に用いられます。
- 知識蒸留の設定(Ba and Caruana 2014; Hinton, Vinyals, and Dean 2015)と同様に，我々は教師モデルによる事後分布と、我々のモデルによって計算された事後分布の間のKLダイバージェンスを利用する．
- さらに，隣接行列$S$(と、それによって潜在グラフ$G_0$)を制御するために，損失関数に2つ目の目的関数を追加する
- 直感的には、脚本の中でシーンが時間的に近い場合には、シーンが$G_0$の隣接ノードになる確率を高くしたいと考えています。
    - このため、損失関数に焦点正則化項Fを追加します。
    - 

### Experiment setup
#### 前処理
- USE(Universal Sentence Encoder):文章レベルの埋め込み表現
- 字幕（およびタイムスタンプ）は、Dynamic Time Wrappingという手法を使用して、脚本の台詞部分に合わせて調整されました。
- その後、脚本のシーンと動画のセグメントのアラインメントを取得した。最後に、ビデオをシーンにセグメンテーションし、視聴覚的特徴を抽出しました。
- シーンの150フレームにつき1フレームをサンプリング
- ImageNetで学習済みのResNeXt-101をつかってフレームごとの画像特徴量を抽出
- 学習済みのYAMNetによって音声特徴量を抽出
- 要約でTPを構成するために3つ連続したシーンを使用
- さらに、スパースで解釈可能なグラフを作成したいので、グラフＧ０のシーンに対して選択可能な隣人Ｃの最大サイズを６に設定する。
- モデルの学習には、式(6)のハイパーパラメータλを10に設定した。
- ネットワークの最適化には、Adamアルゴリズム(Kingma and Ba 2014)を使用した。
- 我々は、脚本のシーンを符号化するために 64 個のニューロンを持つ LSTM を選択し、それらを文脈化するために同じニューロンを選択した。また、0.2のドロップアウトを追加した。私たちのモデルは、PyTorch (Paszke et al. 2019)とPyTorch geometric (Fey and Lenssen 2019)で開発されました。ムービーグラフの分析には、NetworkX（Hagberg, Swart, and S Chult 2008）を使用しました。

### Result
我々の実験は以下の3つの疑問に答えることを目的とした。
- (1) 提案されたグラフベースのモデルは、構造をあまり意識していないモデルと比較して、TPを識別するのに優れているか？
- (2) グラフやマルチモーダル情報はどの程度有用か？
- (3) 自動的に識別されたTPが作成したサマリーは意味を持つのか？

Table2が第一の疑問の答えだ  
38のgold-standardの映画に対して5Foldクロスバリデーションを行い、  
- Total Agreement:正しく選択されたTPシーンの割合
- Partial Agreement:少なくとも1つのgold-standardシーンが確認されたTPイベントの割合  
- Distance:あるTP(1~5)について予測されたシーンとgold-standardシーンのセットの間のシーン数の最小距離を，脚本の長さで正規化したもの（評価基準のより詳細な定義については付録を参照のこと）．
- 比較対象(baseline)
    - 映画の5つの均等に分割されたセクションから3つのシーンのシーケンスをランダムに選択。
    - 脚本理論に基づいて、各TPイベントに位置にすると予想される3つのシーンのシーケンスを選択
    - TRIPODトレーニングセットの説明文におけるgold-standardのTPの位置に基づいて、3つのシーンのシーケンスを選択すること。
- 教師なし要約での比較結果
    - TEXTRANK with neural input representations:
    - SCENESUM:TEXTRANKの派生。シーンの登場人物を考慮。
- TAM:スライディングウィンドウによって連続した文脈の類似度を計算するシーケンスベースの教師モデル(実装の詳細はAppendix)
- すべての比較でaudio-visual特徴も加えたマルチモーダル版での性能も報告する
    - 教師なしモデルでは、GraphTPと同様にシーンレベルの特徴をシーン間の類似度計算に追加の重みとして追加しています。TAMでは、視聴覚情報を追加し、すべてのモダリティからのシーンレベルのベクトルを連結しています。
- 教師なし要約モデル(TEXTRANK, SCENESUM)は、TAとPAの点では競合するが、平均距離Dが有意に高い。これは、彼らが物語のすべての部分からイベントを選択するのではなく、特定の部分からイベントを選択していることを示唆している。
- 教師ありモデル（TAMとGRAPHTP）では、平均距離Dは一般的に低く、位置バイアスに適応し、映画の全部分からイベントを選択することができることを意味している。
- さらに、どちらのモデルもマルチモーダル情報の恩恵を受けているようです（TAとPAのメトリクスを参照）。
- 最後に、GRAPHTPは、より多くのゴールドスタンダードのTPイベントを正しく識別することで、最高のパフォーマンスを発揮しているようです（TAとPAの両方のメトリクスに基づいて）。
